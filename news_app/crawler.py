from openai import OpenAI
import requests
from bs4 import BeautifulSoup
import time
import re
import json
from collections import defaultdict
import os
from my_wordcloud import creat_wordcloud
import matplotlib.pyplot as plt
from playwright.sync_api import sync_playwright 
from urllib.parse import quote
from selenium.webdriver.chrome.options import Options
from selenium import webdriver
import urllib3
import os
import shutil
from draw_pyvis import generate_quadruple_graph_html_with_link
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)



# âœ… æ›¿æ¢æˆä½ çš„ DeepSeek API Key
DEEPSEEK_API_KEY = " "

# âœ… åˆå§‹åŒ– DeepSeek API å®¢æˆ·ç«¯
client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url="https://api.deepseek.com"
)

def print_test(msg):
    print(f"æµ‹è¯•ï¼š{msg}")
    return msg

def safe_request(url, headers=None, timeout=10):
    try:
        headers = headers or {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=timeout)
        response.encoding = response.apparent_encoding
        if response.status_code == 200:
            return response.text
        return None
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {url} - {e}")
        return None

def extract_text_from_tags(soup, selectors):
    for selector in selectors:
        tag = soup.select_one(selector)
        if tag:
            paragraphs = tag.find_all('p')
            text = '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
            if len(text) > 100:
                return text
    return ""

# å‡¤å‡°
def get_fenghuang_news_body(url):
    """æå–å‡¤å‡°æ–°é—»æ­£æ–‡"""
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return ""

        soup = BeautifulSoup(response.text, "html.parser")
        content_divs = soup.select("div.index_smallFont_3Pwv1, div.index_text_TwFCV, div.articleText")
        time_tag = soup.select_one("span.time-3vNLpJrC") or soup.select_one("span.time") or soup.find("time")
        pub_time = time_tag.get_text(strip=True) if time_tag else ""

        content = ""
        for div in content_divs:
            paragraphs = div.find_all("p")
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text:
                    content += text + "\n"

        return content.strip(), pub_time
    except Exception as e:
        print(f"âŒ æ­£æ–‡æå–å¤±è´¥: {url} - {e}")
        return ""

def get_fenghuang_news(keyword: str, max_results: int = 10):
    """æŠ“å–å‡¤å‡°æ–°é—»ï¼Œä¸ç™¾åº¦æ–°é—»å­—æ®µæ ¼å¼ç»Ÿä¸€"""
    results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        search_url = f"https://so.ifeng.com/?q={keyword}"
        page.goto(search_url)
        page.wait_for_timeout(3000)
        
        soup = BeautifulSoup(page.content(), "html.parser")
        browser.close()

    news_list = soup.select("ul.news-stream-basic-news-list li.news_item")

    for li in news_list:
        a_tag = li.select_one("h2 a")
        if not a_tag or not a_tag.get("href"):
            continue

        title = a_tag.get_text(strip=True).replace("\xa0", "")
        url = a_tag.get("href")
        if url.startswith("//"):
            url = "https:" + url

        # ğŸ” è¿›å…¥è¯¦æƒ…é¡µè·å–æ­£æ–‡å’Œæ—¶é—´
        body,time = get_fenghuang_news_body(url)

        results.append({
            "title": title,
            "abstract": "",         # å¯åç»­è¡¥æ‘˜è¦
            "link": url,
            "platform": "å‡¤å‡°ç½‘",
            "time": time,
            "body": body
        })

        if len(results) >= max_results:
            break

    return results
# æ–°æµª
def get_sina_news_body(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        possible_tags = [
            'div.article', 'div.main-content', 'div#artibody', 'div.article-content-left'
        ]
        for tag in possible_tags:
            content = soup.select_one(tag)
            if content:
                paragraphs = content.find_all('p')
                text = '\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
                if len(text) > 100:
                    return text
        return "âš ï¸ æ­£æ–‡è§£æå¤±è´¥"
    except Exception as e:
        return f"âš ï¸ é”™è¯¯ï¼š{str(e)}"

def get_sina_news(keyword, pages=1):
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    for page in range(1, pages + 1):
        url = f"https://search.sina.com.cn/?q={keyword}&c=news&range=all&num=20&page={page}"
        print(f"æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ: {url}")
        
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.select('div.box-result.clearfix')
        
        if not articles:
            print("âš ï¸ æœªæŠ“åˆ°æ–°é—»ï¼Œå¯èƒ½è¢«é™åˆ¶è®¿é—®æˆ–å…³é”®è¯æ— ç»“æœã€‚")
            continue
        for article in articles:
            title_tag = article.select_one('h2 a')
            abstract_tag = article.select_one('p.content')
            time_tag = article.select_one('span.fgray_time')
            if title_tag and abstract_tag and time_tag:
                news_url = title_tag['href']
                news_body = get_sina_news_body(news_url)
                results.append({
                    'title': title_tag.text.strip(),
                    'link': news_url,
                    'abstract': abstract_tag.text.strip(),
                    'time': time_tag.text.strip(),
                    'body': news_body,
                    'platform': 'æ–°æµªæ–°é—»'
                })
        time.sleep(1)
    return results
# ç™¾åº¦
def resolve_real_url(baidu_url):
    """è·³è½¬ç™¾åº¦ä¸­è½¬é“¾æ¥ï¼Œè·å–çœŸå®æ–°é—»URL"""
    try:
        resp = requests.get(baidu_url, headers={"User-Agent": "Mozilla/5.0"},
                            verify=False, timeout=10, allow_redirects=True)
        return resp.url
    except Exception as e:
        print(f"âŒ è·³è½¬å¤±è´¥: {e}")
        return baidu_url

def get_baidu_news_body(url):
    """æŠ“å–æ–°é—»æ­£æ–‡"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10, verify=False)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')

        candidate_selectors = [
            'div.article-content',
            'div.article-body',
            'div.post_text',
            'div#artibody',
            'div#content',
            'div.main-content',
            'div#main-content',
            'div.show_text',
            'div.yd_text',
        ]

        for selector in candidate_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                paragraphs = content_div.find_all('p')
                if paragraphs:
                    return '\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

        # å…œåº•æ–¹æ¡ˆ
        all_paragraphs = soup.find_all('p')
        text = '\n'.join(p.get_text(strip=True) for p in all_paragraphs if p.get_text(strip=True))
        return text if len(text) > 100 else ''
    except Exception as e:
        print(f"âŒ æŠ“æ­£æ–‡å¤±è´¥: {url} -> {e}")
        return ''

def get_baidu_news_with_selenium(keyword, pages=1):
    """ä½¿ç”¨SeleniumæŠ“å–ç™¾åº¦æ–°é—»"""
    # è®¾ç½®Seleniumä¸æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # ä¸æ‰“å¼€æµè§ˆå™¨
    chrome_options.add_argument("--disable-gpu")  # ç¦ç”¨GPU
    driver = webdriver.Chrome(options=chrome_options)

    results = []
    for page in range(pages):
        pn = page * 10
        query = quote(keyword)
        url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&wd={query}&pn={pn}"
        print(f"ğŸ” æŠ“å–ç¬¬ {page + 1} é¡µ: {url}")

        driver.get(url)
        time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½

        # è·å–é¡µé¢å†…å®¹
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # é€‰æ‹©å™¨
        news_blocks = soup.select("div.result") + soup.select("div.result-op")
        for block in news_blocks:
            a_tag = block.select_one('h3 a')
            summary = block.select_one('div.c-abstract') or block.select_one('div.c-span18 p')
            time_tag = block.select_one('span.c-color-gray2') or block.select_one('span.c-color-gray')
            if not a_tag or not a_tag.get('href'):
                continue

            baidu_link = a_tag['href']
            real_url = resolve_real_url(baidu_link)

            body = get_baidu_news_body(real_url)
            if not body:
                continue

            results.append({
                "title": a_tag.get_text(strip=True),
                "abstract": summary.get_text(strip=True) if summary else "",
                "link": real_url,
                "platform": real_url.split('/')[2],
                "time": time_tag.get_text(strip=True) if time_tag else "",
                "body": body,
                'platform': 'ç™¾åº¦æ–°é—»'
            })
            time.sleep(0.5)  # é¿å…è¿‡å¿«

        time.sleep(1)  # ç¿»é¡µå»¶æ—¶
    driver.quit()
    return results

#å…¬å…±æ–¹æ³•
#æ‘˜è¦
def summarize_news(news_body: str) -> list[str]:
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿ä¸­æ–‡æ–°é—»æ€»ç»“å’Œåˆ†ç±»çš„åŠ©æ‰‹ã€‚"
                    "è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–°é—»å†…å®¹ï¼Œ"
                    "è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†çš„å›ç­”ï¼š"
                    "1. æ–°é—»æ‘˜è¦ï¼ˆç®€æ´ï¼Œä¸è¶…è¿‡25å­—ï¼‰ï¼Œ"
                    "2. æ–°é—»åˆ†ç±»ï¼ˆé€šçŸ¥ã€æŠ•è¯‰ã€è¡¨æ‰¬ã€å¤„ç½šã€å»ºè®®ã€å…¶ä»–ï¼‰ï¼Œ"
                    "æ ¼å¼ä¸ºï¼š[æ‘˜è¦å†…å®¹, æ–°é—»åˆ†ç±»]"
                    "æ–°é—»åˆ†ç±»è¯·ä¸¥æ ¼æŒ‰ç…§å…­ç±»è¿›è¡Œåˆ†ç±»ï¼šâ€œé€šçŸ¥ã€æŠ•è¯‰ã€è¡¨æ‰¬ã€å¤„ç½šã€å»ºè®®ã€å…¶ä»–â€ï¼Œä¸å…è®¸ä»»ä½•å…¶ä»–åˆ†ç±»ï¼"
                    
                )
            },
            {"role": "user", "content": f"è¯·æ€»ç»“å¹¶åˆ†ç±»ä»¥ä¸‹æ–°é—»å†…å®¹ï¼š{news_body}"}
        ],
        stream=False
    )
    raw_text = response.choices[0].message.content.strip()
    # è¿™é‡Œå°è¯•è§£ææˆåˆ—è¡¨
    # ç®€å•å¤„ç†ï¼Œå»æ‰ä¸­è‹±æ–‡æ‹¬å·å’Œç©ºæ ¼
    matched = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9ã€‚ï¼Œã€,.]+', raw_text)
    
    if len(matched) >= 2:
        return [matched[0], matched[1]]
    else:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å…¨æ–‡æ‘˜è¦ï¼Œåˆ†ç±»é»˜è®¤æœªåˆ†ç±»
        return [raw_text, "æœªåˆ†ç±»"]
#æå–å››å…ƒç»„
def extract_quadruples(news_body: str):
    prompt = f"""è¯·é˜…è¯»ä»¥ä¸‹æ–°é—»å†…å®¹ï¼ŒæŠ½å–â€œå®ä½“-äº‹ä»¶-å®ä½“-æƒ…æ„Ÿâ€çš„å››å…ƒç»„,è¦é¢å¤–æ³¨æ„è´Ÿé¢æ¶ˆæ¯æˆ–è´Ÿé¢æƒ…ç»ªã€‚
æ¯ä¸ªå››å…ƒç»„åæ˜ ä¸€ä¸ªäº‹å®ï¼š
- å®ä½“1 æ˜¯åŠ¨ä½œæˆ–äº‹ä»¶çš„å‘å‡ºè€…ï¼ˆå¦‚ä¼ä¸šã€æ”¿åºœã€äººç‰©ç­‰ï¼‰ï¼›
- äº‹ä»¶æ˜¯è¡Œä¸ºæˆ–çŠ¶æ€ï¼ˆå¦‚â€œå‘å¸ƒâ€â€œæ‰¹è¯„â€â€œè°ƒæŸ¥â€ï¼‰ï¼›
- å®ä½“2 æ˜¯è¢«å½±å“çš„å¯¹è±¡ï¼ˆå¦‚äº§å“ã€æ”¿ç­–ã€äººç‰©ç­‰ï¼‰ï¼›
- æƒ…æ„Ÿä¸ºæ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§ï¼Œåæ˜ èˆ†è®ºå¯¹æ­¤äº‹ä»¶çš„æ€åº¦ã€‚

è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼é€è¡Œè¾“å‡ºå¤šä¸ªå››å…ƒç»„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼š
(å®ä½“1, äº‹ä»¶, å®ä½“2, æƒ…æ„Ÿ)

ç¤ºä¾‹ï¼š
(åä¸º, å‘å¸ƒ, Pura70, æ­£é¢)
(å°ç±³, è¢«æ‰¹è¯„, äº§å“è´¨é‡, è´Ÿé¢)

è¯·ä¸è¦æ·»åŠ å…¶ä»–è§£é‡Šæ€§å†…å®¹ï¼Œåªè¿”å›æ‹¬å·ä¸­çš„ç»“æ„ã€‚

æ–°é—»å†…å®¹å¦‚ä¸‹ï¼š
{news_body}
    """
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿ä¸­æ–‡ä¿¡æ¯æŠ½å–çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        stream=False
    )
    content = response.choices[0].message.content
    
    if re.search(r'[\x00-\x08\x0B\x0C\x0E-\x1F\uFFFD]', content):
        return ""
    
    return content
#è§£æå››å…ƒç»„
def parse_quadruples(quadruple_text: str|None):
    pattern = re.compile(r'\((.+?),\s*(.+?),\s*(.+?),\s*(.+?)\)')
    matches = pattern.findall(quadruple_text)

    parsed = []
    for ent1, event, ent2, sentiment in matches:
        parsed.append({
            "å®ä½“1": ent1.strip(),
            "äº‹ä»¶": event.strip(),
            "å®ä½“2": ent2.strip(),
            "æƒ…æ„Ÿ": sentiment.strip()
        })
    return parsed
#åç§°å¤„ç†
def safe_filename(title, max_length=50):
    filename = re.sub(r'[\\/*?:"<>|]', '_', title)
    filename = filename.strip().replace(' ', '_')
    return filename[:max_length]
#ä¿å­˜ä¸ºjson
def save_news_json_with_quadruples(news, summary,type, quadruples, output_dir="json_file"):
    os.makedirs(output_dir, exist_ok=True)
    data = {
        "title": news['title'],
        "link": news['link'],
        "time": news['time'],
        "platform": news['platform'],
        "type": type,
        "æ‘˜è¦": summary,
        "å››å…ƒç»„": quadruples
    }
    filename = os.path.join(output_dir, f"{safe_filename(news['title'])}.json")
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def extract_text_from_json(folder_path="sina_news"):
    all_text = []

    for filename in os.listdir(folder_path):
        if filename.endswith(".json"):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                quadruples = data.get("å››å…ƒç»„", [])
                
                for quad in quadruples:
                    ent1 = quad.get("å®ä½“1", "").strip()
                    ent2 = quad.get("å®ä½“2", "").strip()
                    event = quad.get("äº‹ä»¶", "").strip()
                    sentiment = quad.get("æƒ…æ„Ÿ", "").strip()

                    # å¯æŒ‰éœ€é€‰æ‹©ä½¿ç”¨å“ªäº›å­—æ®µ
                    all_text.extend([ent1, ent2, event, sentiment])

    # æ‹¼æ¥ä¸ºä¸€ä¸ªç©ºæ ¼åˆ†éš”çš„é•¿å­—ç¬¦ä¸²ï¼Œä½œä¸ºè¯äº‘è¾“å…¥
    return ' '.join([t for t in all_text if t])  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²

def get_all_news(keyword):
    fenghuang = get_fenghuang_news(keyword)
    sina = get_sina_news(keyword)
    baidu = get_baidu_news_with_selenium(keyword)
    return fenghuang + sina + baidu

# æ¸…ç©ºæ–‡ä»¶å¤¹,é¿å…é‡å¤æŠ“å–
def clear_folder(folder_path):
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(folder_path):
        print(f"è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
        return

    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                # åˆ é™¤æ–‡ä»¶æˆ–ç¬¦å·é“¾æ¥
                os.remove(file_path)
            elif os.path.isdir(file_path):
                # åˆ é™¤æ–‡ä»¶å¤¹åŠå…¶å†…å®¹
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'åˆ é™¤ {file_path} æ—¶å‡ºé”™: {e}')
            
def process_in_one(keyword):
    
    shantou_city=['é¾™æ¹–åŒº','é‡‘å¹³åŒº','æ¿ æ±ŸåŒº','æ½®é˜³åŒº','æ½®å—åŒº','æ¾„æµ·åŒº','å—æ¾³å¿']
    if keyword in shantou_city:
        json_file =  os.path.join('shantou_city', keyword)
        
    else: 
        json_file =os.path.join('shantou_city', keyword)

    # è·å–æ‰€æœ‰æ–°é—»
    news = get_all_news(keyword)
    
    #æ¸…ç©ºå·²æœ‰æ–‡ä»¶å¤¹,åç»­å¯èƒ½è¦ä¼˜åŒ–é€»è¾‘
    clear_folder(json_file)
    
    for i, news in enumerate(news[:30]):
        print(f"\nğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i+1} æ¡æ–°é—»ï¼šã€Š{news['title']}ã€‹")

        # æ‘˜è¦
        get_sum= summarize_news(news['body'])
        summary, type = get_sum[0], get_sum[1]
        
        print(f"æ‘˜è¦ï¼š{summary}")
        print("âœ… æ‘˜è¦å®Œæˆ")

        # æå–å››å…ƒç»„
        triplet_text = extract_quadruples(news['body'])
        if not triplet_text:
            print("âš ï¸ å››å…ƒç»„æå–å¤±è´¥")
            continue
        grouped_triplets = parse_quadruples(triplet_text)
        print("âœ… å››å…ƒç»„æå–å®Œæˆ")
        
        # ä¿å­˜
        save_news_json_with_quadruples(news, summary,type, grouped_triplets,json_file)
        print(f"âœ… å·²ä¿å­˜ä¸º JSON æ–‡ä»¶")
    
    return f"{keyword} æŠ“å–å¹¶å¤„ç†äº†{len(news)} æ¡æ–°é—»ã€‚"
    
if __name__ == "__main__":
    keyword = "æ±•å¤´"
    process_in_one(keyword)